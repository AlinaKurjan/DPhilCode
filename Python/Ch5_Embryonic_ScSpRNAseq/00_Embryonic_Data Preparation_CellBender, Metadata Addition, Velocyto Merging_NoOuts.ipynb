{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf84a32",
   "metadata": {},
   "source": [
    "# CellBender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "\n",
    "# Print date and time:\n",
    "import datetime\n",
    "e = datetime.datetime.now()\n",
    "print (\"Current date and time = %s\" % e)\n",
    "\n",
    "# Set a working directory\n",
    "wdir = \"/mnt/da8aa2c4-0136-465b-87a2-d12a59afec55/akurjan/analysis/files/Teichmann Group/\"\n",
    "os.chdir( wdir )\n",
    "\n",
    "# Set folder structures\n",
    "RESULTS_FOLDERNAME = \"/mnt/da8aa2c4-0136-465b-87a2-d12a59afec55/akurjan/analysis/notebooks/foetal/results/SingleCellQC\"\n",
    "if not os.path.exists(RESULTS_FOLDERNAME):\n",
    "    os.makedirs(RESULTS_FOLDERNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def run_cellbender(working_dir):\n",
    "    for folder in os.listdir(working_dir):\n",
    "        if folder.startswith('cellranger700'):\n",
    "            print(f'PROCESSING FOLDER: {folder}')\n",
    "            \n",
    "            # Check if output files already exist in the folder\n",
    "            output_files_exist = any(file.startswith('cellbender') for file in os.listdir(os.path.join(working_dir, \n",
    "                                                                                                       folder)))\n",
    "            if output_files_exist:\n",
    "                print(f\"Output files already exist in folder {folder}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Process the folder\n",
    "            input_path = os.path.join(working_dir, folder, 'raw_feature_bc_matrix.h5')\n",
    "            output_path = os.path.join(working_dir, folder, 'cellbenderout.h5')\n",
    "            metrics_summary_path = os.path.join(working_dir, folder, 'metrics_summary.csv')\n",
    "            log_file = os.path.join(working_dir, folder, 'cellbender_running.log')\n",
    "\n",
    "            # Extract the estimated number of cells from metrics_summary.csv\n",
    "            metrics_df = pd.read_csv(metrics_summary_path)\n",
    "            expected_cells = float(metrics_df['Estimated Number of Cells'][0].replace(',', ''))\n",
    "            \n",
    "            if expected_cells is not None:\n",
    "                # Run the CellBender subprocess and redirect stdout to log file\n",
    "                command = [\n",
    "                    'cellbender', 'remove-background',\n",
    "                    '--input', input_path,\n",
    "                    '--output', output_path,\n",
    "                    '--expected-cells', str(int(expected_cells)),\n",
    "                    '--total-droplets-included', '30000', # ran with 15000 for all but one sample\n",
    "                    '--fpr', '0.01',\n",
    "                    '--epochs', '150',\n",
    "                    '--cuda'\n",
    "                ]\n",
    "                print(f'Running {command}')\n",
    "                with open(log_file, 'w') as log:\n",
    "                    try:\n",
    "                        result = subprocess.run(command, check=True, stdout=log, \n",
    "                                                stderr=subprocess.PIPE, encoding='utf-8')\n",
    "                    except subprocess.CalledProcessError as e:\n",
    "                        print(f'Error occurred: {e.returncode}\\n{e.stderr}')\n",
    "                    else:\n",
    "                        print(f'Command executed successfully')\n",
    "            else:\n",
    "                print(f\"No estimated number of cells found for {folder}. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cellbender(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f5d0a",
   "metadata": {},
   "source": [
    "# Change to Pyscenic_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71564f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "#import scvelo as scv\n",
    "\n",
    "# Import dependencies\n",
    "import os\n",
    "\n",
    "# Print date and time:\n",
    "import datetime\n",
    "e = datetime.datetime.now()\n",
    "print (\"Current date and time = %s\" % e)\n",
    "\n",
    "# set a working directory\n",
    "wdir = \"/mnt/da8aa2c4-0136-465b-87a2-d12a59afec55/akurjan/analysis/files/Teichmann Group/\"\n",
    "os.chdir( wdir )\n",
    "\n",
    "# folder structures\n",
    "RESULTS_FOLDERNAME = \"/mnt/da8aa2c4-0136-465b-87a2-d12a59afec55/akurjan/analysis/notebooks/foetal/results/SingleCellQC\"\n",
    "if not os.path.exists(RESULTS_FOLDERNAME):\n",
    "    os.makedirs(RESULTS_FOLDERNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import scipy.sparse as sp\n",
    "import anndata\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "def anndata_from_h5(file: str,\n",
    "                    analyzed_barcodes_only: bool = True) -> 'anndata.AnnData':\n",
    "    \"\"\"Load an output h5 file into an AnnData object for downstream work.\n",
    "\n",
    "    Args:\n",
    "        file: The h5 file\n",
    "        analyzed_barcodes_only: False to load all barcodes, so that the size of\n",
    "            the AnnData object will match the size of the input raw count matrix.\n",
    "            True to load a limited set of barcodes: only those analyzed by the\n",
    "            algorithm. This allows relevant latent variables to be loaded\n",
    "            properly into adata.obs and adata.obsm, rather than adata.uns.\n",
    "\n",
    "    Returns:\n",
    "        adata: The anndata object, populated with inferred latent variables\n",
    "            and metadata.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    d = dict_from_h5(file)\n",
    "    X = sp.csc_matrix((d.pop('data'), d.pop('indices'), d.pop('indptr')),\n",
    "                      shape=d.pop('shape')).transpose().tocsr()\n",
    "\n",
    "    # check and see if we have barcode index annotations, and if the file is filtered\n",
    "    barcode_key = [k for k in d.keys() if (('barcode' in k) and ('ind' in k))]\n",
    "    if len(barcode_key) > 0:\n",
    "        max_barcode_ind = d[barcode_key[0]].max()\n",
    "        filtered_file = (max_barcode_ind >= X.shape[0])\n",
    "    else:\n",
    "        filtered_file = True\n",
    "\n",
    "    if analyzed_barcodes_only:\n",
    "        if filtered_file:\n",
    "            # filtered file being read, so we don't need to subset\n",
    "            print('Assuming we are loading a \"filtered\" file that contains only cells.')\n",
    "            pass\n",
    "        elif 'barcode_indices_for_latents' in d.keys():\n",
    "            X = X[d['barcode_indices_for_latents'], :]\n",
    "            d['barcodes'] = d['barcodes'][d['barcode_indices_for_latents']]\n",
    "        elif 'barcodes_analyzed_inds' in d.keys():\n",
    "            X = X[d['barcodes_analyzed_inds'], :]\n",
    "            d['barcodes'] = d['barcodes'][d['barcodes_analyzed_inds']]\n",
    "        else:\n",
    "            print('Warning: analyzed_barcodes_only=True, but the key '\n",
    "                  '\"barcodes_analyzed_inds\" or \"barcode_indices_for_latents\" '\n",
    "                  'is missing from the h5 file. '\n",
    "                  'Will output all barcodes, and proceed as if '\n",
    "                  'analyzed_barcodes_only=False')\n",
    "\n",
    "    # Construct the anndata object.\n",
    "    adata = anndata.AnnData(X=X,\n",
    "                            obs={'barcode': d.pop('barcodes').astype(str)},\n",
    "                            var={'gene_name': (d.pop('gene_names') if 'gene_names' in d.keys()\n",
    "                                               else d.pop('name')).astype(str)},\n",
    "                            dtype=X.dtype)\n",
    "    adata.obs.set_index('barcode', inplace=True)\n",
    "    adata.var.set_index('gene_name', inplace=True)\n",
    "\n",
    "    # For CellRanger v2 legacy format, \"gene_ids\" was called \"genes\"... rename this\n",
    "    if 'genes' in d.keys():\n",
    "        d['id'] = d.pop('genes')\n",
    "\n",
    "    # For purely aesthetic purposes, rename \"id\" to \"gene_id\"\n",
    "    if 'id' in d.keys():\n",
    "        d['gene_id'] = d.pop('id')\n",
    "\n",
    "    # If genomes are empty, try to guess them based on gene_id\n",
    "    if 'genome' in d.keys():\n",
    "        if np.array([s.decode() == '' for s in d['genome']]).all():\n",
    "            if '_' in d['gene_id'][0].decode():\n",
    "                print('Genome field blank, so attempting to guess genomes based on gene_id prefixes')\n",
    "                d['genome'] = np.array([s.decode().split('_')[0] for s in d['gene_id']], dtype=str)\n",
    "\n",
    "    # Add other information to the anndata object in the appropriate slot.\n",
    "    _fill_adata_slots_automatically(adata, d)\n",
    "\n",
    "    # Add a special additional field to .var if it exists.\n",
    "    if 'features_analyzed_inds' in adata.uns.keys():\n",
    "        adata.var['cellbender_analyzed'] = [True if (i in adata.uns['features_analyzed_inds'])\n",
    "                                            else False for i in range(adata.shape[1])]\n",
    "\n",
    "    if analyzed_barcodes_only:\n",
    "        for col in adata.obs.columns[adata.obs.columns.str.startswith('barcodes_analyzed')\n",
    "                                     | adata.obs.columns.str.startswith('barcode_indices')]:\n",
    "            try:\n",
    "                del adata.obs[col]\n",
    "            except Exception:\n",
    "                pass\n",
    "    else:\n",
    "        # Add a special additional field to .obs if all barcodes are included.\n",
    "        if 'barcodes_analyzed_inds' in adata.uns.keys():\n",
    "            adata.obs['cellbender_analyzed'] = [True if (i in adata.uns['barcodes_analyzed_inds'])\n",
    "                                                else False for i in range(adata.shape[0])]\n",
    "\n",
    "    return adata\n",
    "\n",
    "def dict_from_h5(file: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Read in everything from an h5 file and put into a dictionary.\"\"\"\n",
    "    d = {}\n",
    "    with tables.open_file(file) as f:\n",
    "        # read in everything\n",
    "        for array in f.walk_nodes(\"/\", \"Array\"):\n",
    "            d[array.name] = array.read()\n",
    "    return d\n",
    "\n",
    "\n",
    "def _fill_adata_slots_automatically(adata, d):\n",
    "    \"\"\"Add other information to the adata object in the appropriate slot.\"\"\"\n",
    "\n",
    "    for key, value in d.items():\n",
    "        try:\n",
    "            if value is None:\n",
    "                continue\n",
    "            value = np.asarray(value)\n",
    "            if len(value.shape) == 0:\n",
    "                adata.uns[key] = value\n",
    "            elif value.shape[0] == adata.shape[0]:\n",
    "                if (len(value.shape) < 2) or (value.shape[1] < 2):\n",
    "                    adata.obs[key] = value\n",
    "                else:\n",
    "                    adata.obsm[key] = value\n",
    "            elif value.shape[0] == adata.shape[1]:\n",
    "                if value.dtype.name.startswith('bytes'):\n",
    "                    adata.var[key] = value.astype(str)\n",
    "                else:\n",
    "                    adata.var[key] = value\n",
    "            else:\n",
    "                adata.uns[key] = value\n",
    "        except Exception:\n",
    "            print('Unable to load data into AnnData: ', key, value, type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84d5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adata_dict={}\n",
    "for folder in os.listdir(wdir):\n",
    "        if folder.startswith('cellranger700'):\n",
    "            print(f'PROCESSING FOLDER: {folder}')\n",
    "            # extract the sample-specific path to h5 file\n",
    "            h5_filepath = os.path.join(folder, 'cellbenderout_filtered.h5')\n",
    "            # load .h5 file into an anndata object\n",
    "            adata_dict[folder] = anndata_from_h5(file = h5_filepath, analyzed_barcodes_only = True)\n",
    "\n",
    "adata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in adata_dict.keys():\n",
    "    print(f'{key}: {adata_dict[key].n_obs} cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870abda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in adata_dict.keys():\n",
    "    adata_dict[key].write(os.path.join(key, 'cellbenderout_filtered_adata.h5ad'))\n",
    "    print(f'{key} saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81825b36",
   "metadata": {},
   "source": [
    "# CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37351dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5ad_dict={}\n",
    "for folder in os.listdir(wdir):\n",
    "        if folder.startswith('cellranger700'):\n",
    "            # extract the sample-specific path to h5 file\n",
    "            h5ad_filepath = os.path.join(folder, 'cellbenderout_filtered_adata.h5ad')\n",
    "            # load .h5 file into an anndata object\n",
    "            h5ad_dict[folder] = sc.read_h5ad(h5ad_filepath)\n",
    "h5ad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in h5ad_dict.keys():\n",
    "    print(f'{key}: {adata_dict[key].n_obs} .h5 file cells')\n",
    "    print(f'{key}: {h5ad_dict[key].n_obs} .h5ad file cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb52406",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in h5ad_dict.keys():\n",
    "    if h5ad_dict[key].n_obs == adata_dict[key].n_obs:\n",
    "        print('Yass')\n",
    "    else:\n",
    "        print('Oh no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ba7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b05eaa",
   "metadata": {},
   "source": [
    "# Transferring files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocyto_path = '/home/akurjan/Downloads/Velocyto-20230612T135134Z-001/Velocyto'\n",
    "main_path = '/mnt/da8aa2c4-0136-465b-87a2-d12a59afec55/akurjan/analysis/files/Teichmann Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fafb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "for folder in os.listdir(main_path):\n",
    "    match = re.match(r\"cellranger700_count_\\d+_(.*)_[A-Za-z0-9-]+\", folder)\n",
    "    if match:\n",
    "        new_name = match.group(1)\n",
    "        os.rename(os.path.join(main_path, folder), os.path.join(main_path, new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5fa56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_path):\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999cc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(velocyto_path):\n",
    "    if folder.endswith((\"_velocyto\", \"_velocyo\")):\n",
    "        sample_id = folder.rstrip(\"_velocyto\").rstrip(\"_velocyo\")\n",
    "        os.rename(os.path.join(velocyto_path, folder), os.path.join(velocyto_path, sample_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a3747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(velocyto_path):\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7718519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of folder names in velocyto_path\n",
    "velocyto_folders = [folder for folder in os.listdir(velocyto_path) if os.path.isdir(os.path.join(velocyto_path, folder))]\n",
    "\n",
    "# Get the list of folder names in main_path\n",
    "main_folders = [folder for folder in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, folder))]\n",
    "\n",
    "# Check for discrepancies\n",
    "discrepancies = set(velocyto_folders) - set(main_folders)\n",
    "\n",
    "if discrepancies:\n",
    "    print(\"Folder names in velocyto_path that do not have a matching folder in main_path:\")\n",
    "    for folder in discrepancies:\n",
    "        print(folder)\n",
    "else:\n",
    "    print(\"All folder names in velocyto_path have a matching folder in main_path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354036a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_path):\n",
    "    folder_path = os.path.join(main_path, folder)\n",
    "    velocyto_folder_path = os.path.join(folder_path, 'velocyto')\n",
    "\n",
    "    # Create the 'velocyto' subfolder if it doesn't exist\n",
    "    os.makedirs(velocyto_folder_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Created 'velocyto' subfolder in '{folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c51feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def transfer_files(velocyto_path, main_path):\n",
    "    for folder in os.listdir(velocyto_path):\n",
    "        velocyto_folder = os.path.join(velocyto_path, folder)\n",
    "        main_folder = os.path.join(main_path, folder)\n",
    "        velocyto_subfolder = os.path.join(main_folder, 'velocyto')\n",
    "\n",
    "        if os.path.isdir(main_folder) and os.path.isdir(velocyto_subfolder):\n",
    "            # Get the list of files in the velocyto folder\n",
    "            files = os.listdir(velocyto_folder)\n",
    "            \n",
    "            for file in files:\n",
    "                source = os.path.join(velocyto_folder, file)\n",
    "                destination = os.path.join(velocyto_subfolder, file)\n",
    "\n",
    "                # Move the file to the velocyto subfolder\n",
    "                shutil.move(source, destination)\n",
    "\n",
    "                print(f\"Moved '{file}' from '{velocyto_folder}' to '{velocyto_subfolder}'\")\n",
    "        else:\n",
    "            print(f\"Matching folder '{main_folder}' or 'velocyto' subfolder does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8637e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_files(velocyto_path, main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac3c0b",
   "metadata": {},
   "source": [
    "# Merging CellBender and Velocyto outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "\n",
    "# Print date and time:\n",
    "import datetime\n",
    "e = datetime.datetime.now()\n",
    "print (\"Current date and time = %s\" % e)\n",
    "\n",
    "# set a working directory\n",
    "wdir = \"/ceph/project/tendonhca/akurjan/files/Teichmann Group/\"\n",
    "os.chdir( wdir )\n",
    "\n",
    "# folder structures\n",
    "RESULTS_FOLDERNAME = \"/ceph/project/tendonhca/akurjan/analysis/foetal/results/SingleCellQC/\"\n",
    "if not os.path.exists(RESULTS_FOLDERNAME):\n",
    "    os.makedirs(RESULTS_FOLDERNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096553fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "h5ad_dict={}\n",
    "for folder in os.listdir(wdir):\n",
    "    folder_path = os.path.join(wdir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # extract the sample-specific path to h5ad file\n",
    "        h5ad_filepath = os.path.join(wdir, folder, 'cellbenderout_filtered_adata.h5ad')\n",
    "        # load .h5 file into an anndata object\n",
    "        h5ad_dict[folder] = sc.read_h5ad(h5ad_filepath)\n",
    "h5ad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loom_files(adata_dict, data_dir):\n",
    "    \"\"\"\n",
    "    Load spliced/unspliced count loom files into an `ldata_dict` dictionary object from a specified directory.\n",
    "    The loom files should be in a subdirectory for each sample, named as CellRanger_SampleNr_Type_Date.\n",
    "    The hierarchical structure of the directory should be:\n",
    "    data_dir/\n",
    "        CellRanger_DEV16127_Ach_Jan2023/\n",
    "            velocyto/\n",
    "                CellRanger_DEV16127_Ach_Sep2022.loom\n",
    "        CellRanger_OMB1250_Quad_Jan2023/\n",
    "            velocyto/\n",
    "                CellRanger_OMB1250_Quad_Sep2022.loom\n",
    "        ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata_dict : AnnData\n",
    "        Dictionary of AnnData objects split by individual samples.\n",
    "    data_dir : str\n",
    "        Path to the directory containing the loom files for each sample.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ldata_dict : dict\n",
    "        Dictionary containing spliced and unspliced count loom files for each sample, in the same order as `adata_dict` samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a dictionary to store the loaded loom files\n",
    "    ldata_dict = {}\n",
    "    \n",
    "    for sample_dirname, adata in adata_dict.items():\n",
    "    \n",
    "           # construct the path to the loom file\n",
    "            loom_filepath = os.path.join(data_dir, sample_dirname, 'velocyto', f'{sample_dirname}.loom')\n",
    "\n",
    "            # check if the loom file exists before attempting to load it\n",
    "            if os.path.exists(loom_filepath):\n",
    "                # load the loom file into an scvelo AnnData object\n",
    "                ldata = scv.read(loom_filepath, cache=True)\n",
    "\n",
    "                # add the loaded loom data to the dictionary, keyed by the sampletype name\n",
    "                ldata_dict[sample_dirname] = ldata\n",
    "            else:\n",
    "                # if the file is not found, skip it and move to the next one\n",
    "                print(f\"Loom file not found for {loom_filepath}, skipping...\")\n",
    "                continue\n",
    "        \n",
    "    return ldata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992ab8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scvelo as scv \n",
    "\n",
    "ldata_dict = load_loom_files(h5ad_dict, wdir)\n",
    "ldata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c35565",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert h5ad_dict.keys() == ldata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165ae0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in h5ad_dict.keys():\n",
    "    print(f'{key}: {h5ad_dict[key].n_obs} cellbender cells')\n",
    "    print(f'{key}: {ldata_dict[key].n_obs} velocyto cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata_dict['WSSS_THYst9384958'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7870d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_type in ldata_dict.keys():\n",
    "    barcode = ldata_dict[sample_type].obs.index.str.split(':').str[1]\n",
    "    barcode = barcode.str.replace('x', '')\n",
    "    ldata_dict[sample_type].obs['barcode'] = barcode\n",
    "    ldata_dict[sample_type].obs['samplename'] = sample_type\n",
    "    ldata_dict[sample_type].obs.index = barcode + '.' + sample_type\n",
    "    \n",
    "ldata_dict['WSSS_THYst9384958'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e53e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5ad_dict['WSSS_THYst9384958'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc87fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_type in h5ad_dict.keys():\n",
    "    barcode = h5ad_dict[sample_type].obs.index.str.split('-').str[0]\n",
    "    h5ad_dict[sample_type].obs['barcode'] = barcode\n",
    "    h5ad_dict[sample_type].obs['samplename'] = sample_type\n",
    "    h5ad_dict[sample_type].obs.index = barcode + '.' + sample_type\n",
    "    \n",
    "h5ad_dict['WSSS_THYst9384958'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f748e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in h5ad_dict.keys():\n",
    "    names = set(h5ad_dict[key].obs_names)\n",
    "    names2 = set(ldata_dict[key].obs_names)\n",
    "    # Find the overlapping barcodes\n",
    "    overlapping_barcodes = names.intersection(names2)\n",
    "    print(f\"Number of velocyto and cellbender overlapping barcodes for {key}: {len(overlapping_barcodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc25497",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata_dict['WSSS_THYst9384958'].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5ad_dict['WSSS_THYst9384958'].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in ldata_dict.keys():\n",
    "    ldata_dict[keys].var['Gene'] = ldata_dict[keys].var.index\n",
    "    ldata_dict[keys].var.index = ldata_dict[keys].var['Accession']\n",
    "    h5ad_dict[keys].var['gene_name'] = h5ad_dict[keys].var.index\n",
    "    h5ad_dict[keys].var.index = h5ad_dict[keys].var['gene_id']\n",
    "    ldata_dict[keys].var_names_make_unique()\n",
    "    h5ad_dict[keys].var_names_make_unique()\n",
    "    \n",
    "h5ad_dict['WSSS_THYst9384958'].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a321eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = {}\n",
    "for key in ldata_dict.keys():\n",
    "    # merge matrices into original adata objects\n",
    "    mdata = scv.utils.merge(h5ad_dict[key], ldata_dict[key])\n",
    "    merged_dict[key] = mdata\n",
    "\n",
    "merged_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338cfbe",
   "metadata": {},
   "source": [
    "# Metadata addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687324db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del h5ad_dict, ldata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007baaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(wdir, '201015limb_samples_meta.csv'), na_values='NaN')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.iloc[:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[['Sample ID', 'Sample stage', 'Norm. Sample Stage  ', 'Hospital ID', 'Gender', 'kit', 'Sequencing protocol ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name in merged_dict.keys():\n",
    "    # Get the anndata object for the current sample\n",
    "    anndata_obj = merged_dict[sample_name]\n",
    "    \n",
    "    # Match sample names and assign metadata values\n",
    "    metadata_subset = metadata[metadata['Sample ID'] == sample_name]\n",
    "    if not metadata_subset.empty:\n",
    "        metadata_values = metadata_subset[['Sample stage', 'Norm. Sample Stage  ', 'Hospital ID', 'Gender', 'kit', 'Sequencing protocol ']].values[0]\n",
    "        \n",
    "        # Assign metadata values to the anndata object's .obs attribute\n",
    "        anndata_obj.obs['sample_stage'] = metadata_values[0]\n",
    "        anndata_obj.obs['norm_sample_stage'] = metadata_values[1]\n",
    "        anndata_obj.obs['hospital_id'] = metadata_values[2]\n",
    "        anndata_obj.obs['sex'] = metadata_values[3]\n",
    "        anndata_obj.obs['kit'] = metadata_values[4]\n",
    "        anndata_obj.obs['seq_protocol'] = metadata_values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict['WSSS_THYst8796442'].obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa038a",
   "metadata": {},
   "source": [
    "# Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefiles(merged_dict):\n",
    "    for key in merged_dict.keys():\n",
    "        filename = f\"{key}_unfiltered.h5ad\"\n",
    "        filepath = os.path.join(RESULTS_FOLDERNAME, filename)\n",
    "        merged_dict[key].write(filepath)\n",
    "        print(f\"Saved file {filename} to {RESULTS_FOLDERNAME}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7086d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savefiles(merged_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scvi-env]",
   "language": "python",
   "name": "conda-env-scvi-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
